{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow Analysis - DDMD\n",
    "\n",
    "This notebook provides a simplified interface to the workflow analysis using modular Python scripts.\n",
    "\n",
    "## Overview\n",
    "The analysis process includes:\n",
    "1. Loading workflow data from datalife statistics\n",
    "2. Estimating transfer rates using 4D interpolation from IOR benchmark data\n",
    "3. Calculating Storage Performance Metrics (SPM) for different storage configurations\n",
    "4. Generating visualizations and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Import our workflow analysis modules\n",
    "from modules.workflow_config import DEFAULT_WF, TEST_CONFIGS, STORAGE_LIST\n",
    "from modules.workflow_data_utils import (\n",
    "    load_workflow_data, calculate_io_time_breakdown\n",
    ")\n",
    "from modules.workflow_interpolation import (\n",
    "    estimate_transfer_rates_for_workflow, calculate_aggregate_filesize_per_node\n",
    ")\n",
    "from modules.workflow_spm_calculator import (\n",
    "    calculate_spm_for_workflow, filter_storage_options,\n",
    "    display_top_sorted_averaged_rank, select_best_storage_and_parallelism\n",
    ")\n",
    "from modules.workflow_visualization import plot_all_visualizations\n",
    "from modules.workflow_data_staging import insert_data_staging_rows\n",
    "\n",
    "print(\"Modules imported successfully using modules package!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the workflow to analyze and other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# 1kg ddmd_4n_l\n",
    "WORKFLOW_NAME = \"1kg_2\"  # Change this to analyze different workflows \n",
    "IOR_DATA_PATH = \"../perf_profiles/updated_master_ior_df.csv\"\n",
    "\n",
    "print(f\"Analyzing workflow: {WORKFLOW_NAME}\")\n",
    "print(f\"Available workflows: {list(TEST_CONFIGS.keys())}\")\n",
    "print(f\"IOR data path: {IOR_DATA_PATH}\")\n",
    "\n",
    "ALLOWED_PARALLELISM = TEST_CONFIGS[WORKFLOW_NAME][\"ALLOWED_PARALLELISM\"]\n",
    "print(f\"Allowed parallelism: {ALLOWED_PARALLELISM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Workflow Data\n",
    "\n",
    "Load and process the workflow data from datalife statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load workflow data\n",
    "print(\"Loading workflow data...\")\n",
    "wf_df, task_order_dict, all_wf_dict = load_workflow_data(WORKFLOW_NAME, debug=False)\n",
    "\n",
    "print(f\"\\nWorkflow data loaded:\")\n",
    "print(f\"- Total records: {len(wf_df)}\")\n",
    "print(f\"- Task definitions: {len(task_order_dict)}\")\n",
    "print(f\"- Unique tasks: {list(wf_df['taskName'].unique())}\")\n",
    "print(f\"- Stages: {sorted(wf_df['stageOrder'].unique())}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"\\nFirst few rows of workflow data:\")\n",
    "print(wf_df.head())\n",
    "print(wf_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calculate I/O Time Breakdown\n",
    "\n",
    "Calculate the I/O time breakdown for each task in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get configuration for the workflow\n",
    "config = TEST_CONFIGS[WORKFLOW_NAME]\n",
    "num_nodes_list = config[\"NUM_NODES_LIST\"]\n",
    "\n",
    "# Create task name to parallelism mapping\n",
    "task_name_to_parallelism = {task: info['parallelism'] for task, info in task_order_dict.items()}\n",
    "\n",
    "# Calculate I/O time breakdown\n",
    "print(\"Calculating I/O time breakdown...\")\n",
    "io_breakdown = calculate_io_time_breakdown(wf_df, task_name_to_parallelism, num_nodes_list)\n",
    "\n",
    "print(f\"\\nI/O breakdown results:\")\n",
    "for key, value in io_breakdown.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"{key}:\")\n",
    "        for sub_key, sub_value in value.items():\n",
    "            print(f\"  {sub_key}: {sub_value:.2f} seconds\")\n",
    "    else:\n",
    "        print(f\"{key}: {value:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: Calculate Aggregate File Size per Node\n",
    "\n",
    "Calculate the aggregate file size per node for proper scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate aggregate file size per node\n",
    "print(\"Calculating aggregate file size per node...\")\n",
    "wf_df = calculate_aggregate_filesize_per_node(wf_df)\n",
    "\n",
    "print(\"\\nAggregate file size calculation completed.\")\n",
    "print(f\"Updated columns: {[col for col in wf_df.columns if 'aggregateFilesizeMB' in col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display rows with taskName include string \"stage-\"\n",
    "# staged = insert_data_staging_rows(wf_df)\n",
    "# print(staged[staged['taskName'].str.startswith('stage_')][['taskName', 'stageOrder', 'operation']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2: Insert data movement steps to workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.2: Insert data staging (I/O) rows into the workflow DataFrame\n",
    "\n",
    "# Set debug=False to see detailed output, or False for silent operation\n",
    "wf_df = insert_data_staging_rows(wf_df, debug=False)\n",
    "\n",
    "print(\"Data staging rows inserted. New DataFrame shape:\", wf_df.shape)\n",
    "display(wf_df.head(10))\n",
    "\n",
    "# save the updated wf_df to a csv file\n",
    "wf_df.to_csv(f\"./analysis_data/{WORKFLOW_NAME}_wf_df_with_staging.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_df = pd.read_csv(f\"./analysis_data/{WORKFLOW_NAME}_wf_df_with_staging.csv\")\n",
    "saved_none_rows = saved_df[saved_df['operation'] == 'none']\n",
    "print(f\"\\nSaved file has {len(saved_none_rows)} rows with operation='none'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"\\nFirst few rows of workflow data:\")\n",
    "print(wf_df.head())\n",
    "print(wf_df.columns)\n",
    "print(wf_df[wf_df['operation'] == 0]['taskName'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load IOR Benchmark Data and Estimate Transfer Rates\n",
    "\n",
    "Load the IOR benchmark data and estimate transfer rates for different storage configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IOR benchmark data\n",
    "print(\"Loading IOR benchmark data...\")\n",
    "if os.path.exists(IOR_DATA_PATH):\n",
    "    df_ior = pd.read_csv(IOR_DATA_PATH)\n",
    "    print(f\"Loaded {len(df_ior)} IOR benchmark records\")\n",
    "\n",
    "    # Estimate transfer rates\n",
    "    print(\"\\nEstimating transfer rates...\")\n",
    "    cp_scp_parallelism = set(wf_df.loc[wf_df['operation'].isin(['cp', 'scp']), 'parallelism'].unique())\n",
    "    ALLOWED_PARALLELISM = sorted(set(ALLOWED_PARALLELISM).union(cp_scp_parallelism))\n",
    "\n",
    "    # Then call the function:\n",
    "    wf_df = estimate_transfer_rates_for_workflow(\n",
    "        wf_df, df_ior, STORAGE_LIST, ALLOWED_PARALLELISM, multi_nodes=True, debug=False\n",
    "    )\n",
    "    # wf_df = estimate_transfer_rates_for_workflow(wf_df, df_ior, STORAGE_LIST, ALLOWED_PARALLELISM)\n",
    "    print(\"Transfer rate estimation completed\")\n",
    "    \n",
    "    # Show estimated transfer rate columns\n",
    "    estimated_cols = [col for col in wf_df.columns if col.startswith('estimated_trMiB_')]\n",
    "    print(f\"\\nEstimated transfer rate columns: {len(estimated_cols)}\")\n",
    "    print(f\"Sample columns: {estimated_cols}\")\n",
    "else:\n",
    "    print(f\"Warning: IOR data file not found at {IOR_DATA_PATH}\")\n",
    "    print(\"Skipping transfer rate estimation...\")\n",
    "    df_ior = pd.DataFrame()\n",
    "\n",
    "# Save the estimated transfer rates to a new CSV file\n",
    "os.makedirs(\"./analysis_data\", exist_ok=True)\n",
    "wf_df.to_csv(f\"./analysis_data/{WORKFLOW_NAME}_estimated_transfer_rates.csv\", index=True)\n",
    "print(f\"Saved estimated transfer rates to: ./analysis_data/{WORKFLOW_NAME}_estimated_transfer_rates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Show full width\n",
    "pd.set_option('display.width', None)\n",
    "# Don't truncate column content\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Calculate SPM Values\n",
    "\n",
    "Calculate Storage Performance Metrics (SPM) for the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SPM values\n",
    "print(\"Calculating SPM values...\")\n",
    "# Set debug=False for verbose output, debug=False for minimal output\n",
    "spm_results = calculate_spm_for_workflow(wf_df, debug=True)\n",
    "\n",
    "print(f\"\\nSPM calculation completed:\")\n",
    "print(f\"- Producer-consumer pairs: {len(spm_results)}\")\n",
    "for pair in spm_results.keys():\n",
    "    print(f\"  - {pair}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printe all spm results with producer : consumer pairs\n",
    "\n",
    "# Print weighted SPM values for debugging\n",
    "for pair, data in spm_results.items():\n",
    "    # # only print producer and consumer that are not stage_in and stage_out\n",
    "    # if 'stage_in' in pair or 'stage_out' in pair:\n",
    "    #     continue\n",
    "\n",
    "    print(f\"\\nProducer-Consumer Pair: {pair}\")\n",
    "    print(\"SPM:\")\n",
    "    for storage_n, spm_values in data['SPM'].items():\n",
    "        print(f\"  {storage_n}: {spm_values[0:10]} ...\")\n",
    "    print(\"estT_prod:\")\n",
    "    for storage_n, estT_prod_values in data['estT_prod'].items():\n",
    "        print(f\"  {storage_n}: {estT_prod_values[0:10]} ...\")\n",
    "    print(\"estT_cons:\")\n",
    "    for storage_n, estT_cons_values in data['estT_cons'].items():\n",
    "        print(f\"  {storage_n}: {estT_cons_values[0:10]} ...\")\n",
    "    print(\"dsize_prod:\")\n",
    "    for storage_n, dsize_prod_values in data['dsize_prod'].items():\n",
    "        print(f\"  {storage_n}: {dsize_prod_values[0:10]} ...\")\n",
    "    print(\"dsize_cons:\")\n",
    "    for storage_n, dsize_cons_values in data['dsize_cons'].items():\n",
    "        print(f\"  {storage_n}: {dsize_cons_values[0:10]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Filter Storage Options and Select Best Configuration\n",
    "\n",
    "Filter storage options and select the best storage configuration for each producer-consumer pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter storage options\n",
    "print(\"Filtering storage options...\")\n",
    "filtered_spm_results = filter_storage_options(spm_results, WORKFLOW_NAME)\n",
    "\n",
    "# Select best storage and parallelism\n",
    "print(\"\\nSelecting best storage and parallelism...\")\n",
    "best_results = select_best_storage_and_parallelism(spm_results, baseline=0)\n",
    "\n",
    "print(\"\\nBest storage configurations:\")\n",
    "for pair, config in best_results.items():\n",
    "    print(f\"{pair}:\")\n",
    "    print(f\"  Best storage: {config['best_storage_type']}\")\n",
    "    print(f\"  Best parallelism: {config['best_parallelism']}\")\n",
    "    print(f\"  Best rank: {config['best_rank']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Display Top Results\n",
    "\n",
    "Display the top storage configurations based on rank values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top results\n",
    "print(\"Displaying top results...\")\n",
    "display_top_sorted_averaged_rank(spm_results, top_n=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Generate Visualizations\n",
    "\n",
    "Generate comprehensive visualizations of the analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate visualizations\n",
    "# print(\"Generating visualizations...\")\n",
    "# plot_all_visualizations(wf_df, best_results, io_breakdown['task_io_time_adjust'])\n",
    "# print(\"Visualizations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Results\n",
    "\n",
    "Save the analysis results to files for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME Step 9: Debug and save filtered SPM results\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from modules.workflow_results_exporter import extract_producer_consumer_results, print_storage_analysis\n",
    "\n",
    "# Debug: Check what's in filtered_spm_results\n",
    "print(\"=== Debugging filtered_spm_results ===\")\n",
    "print(f\"Type: {type(filtered_spm_results)}\")\n",
    "print(f\"Length: {len(filtered_spm_results) if filtered_spm_results else 'None/Empty'}\")\n",
    "\n",
    "if filtered_spm_results:\n",
    "    print(f\"Keys: {list(filtered_spm_results.keys())[:5]}...\")  # First 5 keys\n",
    "    \n",
    "    # Examine first item structure\n",
    "    first_key = list(filtered_spm_results.keys())[0]\n",
    "    first_value = filtered_spm_results[first_key]\n",
    "    print(f\"\\nFirst item - Key: '{first_key}'\")\n",
    "    print(f\"  Type: {type(first_value)}\")\n",
    "    if isinstance(first_value, dict):\n",
    "        print(f\"  Keys: {list(first_value.keys())}\")\n",
    "        for k, v in first_value.items():\n",
    "            print(f\"    {k}: {type(v)} = {v}\")\n",
    "\n",
    "# Try to extract results\n",
    "print(\"\\n=== Attempting to Extract Results ===\")\n",
    "try:\n",
    "    results_df = extract_producer_consumer_results(filtered_spm_results, wf_df)\n",
    "    print(f\"Extracted {len(results_df)} rows\")\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"Sample data:\")\n",
    "        print(results_df.head())\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_dir = \"workflow_spm_results\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        workflow_name = \"ddmd_4n_l\"  # or your workflow name\n",
    "        csv_filename = f\"{workflow_name}_filtered_spm_results.csv\"\n",
    "        csv_path = os.path.join(output_dir, csv_filename)\n",
    "        \n",
    "        results_df.to_csv(csv_path, index=False)\n",
    "        print(f\"✓ Saved to: {csv_path}\")\n",
    "        \n",
    "        # Print storage analysis\n",
    "        print_storage_analysis(results_df)\n",
    "        \n",
    "    else:\n",
    "        print(\"Error: Extracted DataFrame is empty - trying alternative method...\")\n",
    "        \n",
    "        # Alternative extraction method\n",
    "        results_data = []\n",
    "        for pair, data in filtered_spm_results.items():\n",
    "            if isinstance(data, dict):\n",
    "                # Try to find storage and SPM information\n",
    "                storage_info = None\n",
    "                spm_value = None\n",
    "                \n",
    "                # Look for storage-related keys\n",
    "                for key in data.keys():\n",
    "                    if 'storage' in key.lower() or 'type' in key.lower():\n",
    "                        storage_info = data[key]\n",
    "                    if 'spm' in key.lower() or 'rank' in key.lower():\n",
    "                        spm_value = data[key]\n",
    "                \n",
    "                if storage_info:\n",
    "                    producer, consumer = pair.split(':') if ':' in pair else ('unknown', 'unknown')\n",
    "                    results_data.append({\n",
    "                        'producer': producer,\n",
    "                        'producerStage': -1,\n",
    "                        'consumer': consumer,\n",
    "                        'consumerStage': -1,\n",
    "                        'prodParallelism': np.nan,\n",
    "                        'consParallelism': np.nan,\n",
    "                        'p-c-Storage': storage_info,\n",
    "                        'p-c-SPM': spm_value if spm_value else np.nan\n",
    "                    })\n",
    "        \n",
    "        if results_data:\n",
    "            alt_df = pd.DataFrame(results_data)\n",
    "            \n",
    "            # Fill stage information\n",
    "            task_stage_mapping = {}\n",
    "            for _, row in wf_df.iterrows():\n",
    "                task_name = row['taskName']\n",
    "                stage_order = row['stageOrder']\n",
    "                if task_name not in task_stage_mapping:\n",
    "                    task_stage_mapping[task_name] = stage_order\n",
    "            \n",
    "            for i, row in alt_df.iterrows():\n",
    "                if row['producer'] in task_stage_mapping:\n",
    "                    alt_df.at[i, 'producerStage'] = task_stage_mapping[row['producer']]\n",
    "                if row['consumer'] in task_stage_mapping:\n",
    "                    alt_df.at[i, 'consumerStage'] = task_stage_mapping[row['consumer']]\n",
    "            \n",
    "            # Save alternative results\n",
    "            output_dir = \"workflow_spm_results\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            workflow_name = \"ddmd_4n_l\"\n",
    "            csv_filename = f\"{workflow_name}_filtered_spm_results_alt.csv\"\n",
    "            csv_path = os.path.join(output_dir, csv_filename)\n",
    "            \n",
    "            alt_df.to_csv(csv_path, index=False)\n",
    "            print(f\"✓ Saved alternative results to: {csv_path}\")\n",
    "            print(f\"Alternative DataFrame shape: {alt_df.shape}\")\n",
    "            print(alt_df.head())\n",
    "        else:\n",
    "            print(\"Error: No data could be extracted with alternative method\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The workflow analysis has been completed successfully. The results include:\n",
    "\n",
    "1. **Workflow Data**: Processed datalife statistics organized in a DataFrame\n",
    "2. **I/O Breakdown**: Time analysis for each task in the workflow\n",
    "3. **Transfer Rate Estimates**: Estimated transfer rates for different storage configurations\n",
    "4. **SPM Values**: Storage Performance Metrics for producer-consumer pairs\n",
    "5. **Best Configurations**: Recommended storage and parallelism settings\n",
    "6. **Visualizations**: Comprehensive plots and charts\n",
    "7. **Saved Results**: CSV and JSON files for future reference\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "The analysis provides insights into:\n",
    "- Which storage types perform best for each workflow stage\n",
    "- Optimal parallelism levels for different storage configurations\n",
    "- I/O bottlenecks and performance characteristics\n",
    "- Recommendations for storage selection\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "You can:\n",
    "- Analyze different workflows by changing the `WORKFLOW_NAME` variable\n",
    "- Modify the analysis parameters in the configuration\n",
    "- Use the saved results for further analysis or comparison\n",
    "- Run the analysis programmatically using the `workflow_analysis_main.py` script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
