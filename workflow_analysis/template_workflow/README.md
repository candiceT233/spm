# Template Workflow - Detailed Documentation

This directory contains a template workflow system designed for testing, development, and learning the workflow analysis pipeline. The template demonstrates a simple producer-consumer relationship with artificial data that mimics real workflow patterns.

## üöÄ Using SPM with User-Defined Workflows

To use SPM (Storage Performance Matching) with user-defined workflows that have trace data other than datalife I/O traces, users need two key components:

### 1. **Script Order JSON Structure**
Define your workflow structure in a JSON file that specifies:
- Task names and execution order
- Parallelism and node configurations  
- File input/output patterns using regex
- Producer-consumer relationships

**Example Structure:**
```json
{
    "task1": {
        "stage_order": 0,
        "parallelism": 4,
        "num_tasks": 4,
        "predecessors": {
            "initial_data": {
                "inputs": ["input_data_\\d+\\.txt"]
            }
        },
        "outputs": [
            "task1_output_\\d+\\.dat"
        ]
    },
    "task2": {
        "stage_order": 1,
        "parallelism": 2,
        "num_tasks": 2,
        "predecessors": {
            "task1": {
                "inputs": ["task1_output_\\d+\\.dat"]
            }
        },
        "outputs": [
            "final_result_\\d+\\.out"
        ]
    }
}
```

### 2. **Workflow CSV Data**
Create a CSV file containing your workflow trace data with the required columns:

**Required Columns:**
- `operation`: 0=write, 1=read (workflow data format)
- `totalTime`: Processing time in seconds
- `aggregateFilesizeMB`: File size in MB
- `parallelism`: Number of parallel tasks
- `taskName`: Name of the task
- `fileName`: Name of the file being processed
- `stageOrder`: Execution stage order
- `storageType`: Storage system type

**Example CSV Structure:**
```csv
operation,totalTime,aggregateFilesizeMB,parallelism,taskName,fileName,stageOrder,storageType
1,8.53,94.51,4,task1,input_data_1.txt,0,beegfs
0,6.12,85.23,4,task1,task1_output_1.dat,0,beegfs
1,12.45,78.91,2,task2,task1_output_1.dat,1,beegfs
0,9.67,92.34,2,task2,final_result_1.out,1,beegfs
```

### 3. **Generate Template Example**
An example template can be generated by running the template generator:

```bash
# Generate default template
python3 modules/workflow_template_generator.py

# Generate with custom CSV filename
python3 modules/workflow_template_generator.py --csv-filename my_workflow.csv

# Generate with custom workflow name
python3 modules/workflow_template_generator.py --workflow-name my_custom_workflow --csv-filename my_data.csv
```

### 4. **Directory Structure**
Place your files in the correct directory structure:
```
your_workflow/
‚îú‚îÄ‚îÄ your_script_order.json
‚îî‚îÄ‚îÄ your_test_folder/
    ‚îî‚îÄ‚îÄ t1/
        ‚îú‚îÄ‚îÄ your_workflow.csv
        ‚îú‚îÄ‚îÄ input_files.txt
        ‚îî‚îÄ‚îÄ output_files.dat
```

### 5. **Configuration**
**Important**: You must add your workflow to the configuration dictionary in `modules/workflow_config.py`. This is required for the system to find and load your workflow data.

Add your workflow to the `TEST_CONFIGS` dictionary in `modules/workflow_config.py`:
```python
"your_workflow": {
    "SCRIPT_ORDER": "your_script_order",
    "NUM_NODES_LIST": [4],
    "ALLOWED_PARALLELISM": [1, 2, 4, 8],
    "exp_data_path": "./your_workflow",
    "test_folders": ["your_test_folder"]
}
```

**Required Configuration Fields:**
- `SCRIPT_ORDER`: Name of your script order JSON file (without .json extension)
- `NUM_NODES_LIST`: List of node configurations to test
- `ALLOWED_PARALLELISM`: List of parallelism levels to analyze
- `exp_data_path`: Path to your workflow data directory
- `test_folders`: List of test folder names containing your workflow data

**Note**: Without adding your workflow to this configuration dictionary, the system will not be able to locate or analyze your workflow data.

### 6. **Run Analysis**
Analyze your workflow using the SPM system:
```bash
# Analyze with default CSV filename
python3 workflow_analysis_main.py --workflow your_workflow

# Analyze with custom CSV filename
python3 workflow_analysis_main.py --workflow your_workflow --csv-filename my_workflow.csv
```

### Key Benefits
- **Flexible Data Format**: Works with any CSV trace data, not just datalife traces
- **Custom Workflow Support**: Define your own workflow structure and dependencies
- **SPM Analysis**: Get storage performance recommendations for your specific workflow
- **Template Generation**: Use the template generator as a starting point for your own workflows

## üìÅ Directory Structure

```
template_workflow/
‚îú‚îÄ‚îÄ README.md                        # This documentation
‚îú‚îÄ‚îÄ template_script_order.json       # Workflow configuration
‚îî‚îÄ‚îÄ template_t1/                     # Workflow execution data
    ‚îî‚îÄ‚îÄ t1/
        ‚îú‚îÄ‚îÄ workflow_data.csv        # Artificial workflow data
        ‚îú‚îÄ‚îÄ input_data_*.txt         # Example input files
        ‚îú‚îÄ‚îÄ task1_output_*.dat       # Example task1 output files
        ‚îî‚îÄ‚îÄ final_result_*.out       # Example final output files
```

## üîß Template Configuration

The `template_script_order.json` file defines the workflow structure with tasks, parallelism, and file patterns. The `workflow_data.csv` contains artificial workflow data with performance metrics.

## üìä Data Format

### CSV Structure
The workflow CSV contains columns for operation type (0=write, 1=read), timing, file sizes, parallelism, task names, and storage types.

### Key Metrics
- **Operation Types**: 0=write, 1=read (workflow data format)
- **Performance**: totalTime, aggregateFilesizeMB, trMiB (transfer rate)
- **Configuration**: parallelism, numTasks, storageType
- **Dependencies**: taskName, stageOrder, prevTask

## üîÑ Workflow Execution

### Stage 0: Task1 (Producer)
- Reads from `input_data_*.txt` files
- Writes `task1_output_*.dat` files
- 4 parallel tasks

### Stage 1: Task2 (Consumer)  
- Reads from `task1_output_*.dat` files
- Writes `final_result_*.out` files
- 2 parallel tasks

## üõ†Ô∏è Usage Examples

### Generate Template
```bash
# Default template
python3 modules/workflow_template_generator.py

# Custom filename
python3 modules/workflow_template_generator.py --csv-filename my_workflow.csv

# Custom workflow name
python3 modules/workflow_template_generator.py --workflow-name my_workflow --csv-filename my_data.csv
```

### Run Analysis
```bash
# Basic analysis
python3 workflow_analysis_main.py --workflow template_workflow

# With custom CSV filename
python3 workflow_analysis_main.py --workflow template_workflow --csv-filename my_workflow.csv

# With debug output
python3 workflow_analysis_main.py --workflow template_workflow --debug
```

## üìö Related Documentation

- **Main README**: `../README.md` - Overview of the workflow analysis system
- **Module Documentation**: `../modules/README.md` - Detailed module descriptions
- **API Reference**: `../modules/workflow_template_generator.py` - Template generator API
- **Configuration**: `../modules/workflow_config.py` - System configuration

---

This template workflow provides a foundation for understanding and testing the workflow analysis system. Use it as a starting point for creating custom workflows or learning the analysis pipeline. 